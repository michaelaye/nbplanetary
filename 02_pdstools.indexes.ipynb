{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pdstools.indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-journal",
   "metadata": {},
   "source": [
    "# PDSTools Indexes\n",
    "\n",
    "> Support tools to work with PDS ISS indexfiles. \n",
    "\n",
    "## IndexLabel\n",
    "\n",
    "The main user interface is the IndexLabel class which is able to load the table file for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "\n",
    "import pandas as pd\n",
    "import pvl\n",
    "import toml\n",
    "from dateutil import parser\n",
    "from planetarypy import utils\n",
    "from planetarypy.config import config\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from .scraper import CTXIndex\n",
    "\n",
    "try:\n",
    "    # 3.6 compatibility\n",
    "    from importlib_resources import path as resource_path\n",
    "except ModuleNotFoundError:\n",
    "    from importlib.resources import path as resource_path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "indices_root = Path(config.data_archive) / \"indices\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/maye/big_drive/planetary_data/planetarypy/indices')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-basket",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "@dataclass\n",
    "class Index:\n",
    "    \"\"\"Index manager class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : str\n",
    "        Nested key in form of mission.instrument.index_name\n",
    "    url : str\n",
    "        URL to index\n",
    "    timestamp : str\n",
    "        Timestamp in ISO time format yy-mm-ddTHH:MM:SS.\n",
    "        This is usually read by the IndexDB class from the config file and its\n",
    "        value is the time of the last download.\n",
    "    \"\"\"\n",
    "\n",
    "    local_root = indices_root\n",
    "    key: str\n",
    "    url: str\n",
    "    timestamp: str\n",
    "\n",
    "    @property\n",
    "    def needs_download(self):\n",
    "        \"\"\"Determine if the index needs to be downloaded.\n",
    "\n",
    "        Download shall happen when (1) no local timestamp was stored or (2) when the remote timestamp\n",
    "        is newer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : indices.Index (namedtuple)\n",
    "            Index holding the timestamp attribute read from the config file\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Boolean indicating if download shall happen.\n",
    "        \"\"\"\n",
    "        remote_timestamp = utils.get_remote_timestamp(self.url)\n",
    "        self.new_timestamp = remote_timestamp\n",
    "        if self.timestamp:\n",
    "            if remote_timestamp > parser.parse(self.timestamp):\n",
    "                return True\n",
    "        else:\n",
    "            # also return True when the timestamp is not valid\n",
    "            return True\n",
    "        # all other cases no D/L required\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def key_tokens(self):\n",
    "        return self.key.split(\".\")\n",
    "\n",
    "    @property\n",
    "    def mission(self):\n",
    "        return self.key_tokens[0]\n",
    "\n",
    "    @property\n",
    "    def instrument(self):\n",
    "        return self.key_tokens[1]\n",
    "\n",
    "    @property\n",
    "    def index_name(self):\n",
    "        \"str: Examples: EDR, RDR\"\n",
    "        return self.key_tokens[2]\n",
    "\n",
    "    @property\n",
    "    def label_filename(self):\n",
    "        return Path(self.url.split(\"/\")[-1])\n",
    "\n",
    "    @property\n",
    "    def isupper(self):\n",
    "        return self.label_filename.suffix.isupper()\n",
    "\n",
    "    @property\n",
    "    def table_filename(self):\n",
    "        new_suffix = \".TAB\" if self.isupper else \".tab\"\n",
    "        return self.label_filename.with_suffix(new_suffix)\n",
    "\n",
    "    @property\n",
    "    def label_path(self):\n",
    "        return Path(urlsplit(self.url).path)\n",
    "\n",
    "    @property\n",
    "    def table_path(self):\n",
    "        return self.label_path.with_name(self.table_filename.name)\n",
    "\n",
    "    @property\n",
    "    def table_url(self):\n",
    "        tokens = urlsplit(self.url)\n",
    "        return urlunsplit(\n",
    "            tokens._replace(\n",
    "                path=str(self.label_path.with_name(self.table_filename.name))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def local_dir(self):\n",
    "        p = self.local_root / f\"{self.mission}/{self.instrument}/{self.index_name}\"\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def local_table_path(self):\n",
    "        return self.local_dir / self.table_filename\n",
    "\n",
    "    @property\n",
    "    def local_label_path(self):\n",
    "        return self.local_dir / self.label_filename\n",
    "\n",
    "    @property\n",
    "    def local_hdf_path(self):\n",
    "        return self.local_table_path.with_suffix(\".hdf\")\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return pd.read_hdf(self.local_hdf_path)\n",
    "\n",
    "    def download(self, local_dir=\"\", convert_to_hdf=True):\n",
    "        \"\"\"Wrapping URLs for downloading PDS indices and their label files.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key : str, optional\n",
    "            Period-separated key into the available index files, e.g. cassini.uvis.moon_summary\n",
    "        label_url : str, optional\n",
    "            Alternative to using the index system, the user can provide the URL to a label\n",
    "            for an index. The table file has to be in the same folder, as usual.\n",
    "        local_dir: str, pathlib.Path, optional\n",
    "            Path for local storage. Default: current directory and filename from URL\n",
    "        convert_to_hdf : bool\n",
    "            Switch to convert the index automatically to a faster loading HDF file\n",
    "        \"\"\"\n",
    "        if not local_dir:\n",
    "            local_dir = self.local_dir\n",
    "        # check timestamp\n",
    "        if not self.needs_download:\n",
    "            print(\"Stored index is up-to-date.\")\n",
    "            return\n",
    "        label_url = self.url\n",
    "        logger.info(\"Downloading %s.\" % label_url)\n",
    "        local_label_path, _ = utils.download(label_url, local_dir)\n",
    "        logger.info(\"Downloading %s.\", self.table_url)\n",
    "        local_data_path, _ = utils.download(self.table_url, local_dir)\n",
    "        IndexDB().update_timestamp(self)\n",
    "        if convert_to_hdf is True:\n",
    "            self.convert_to_hdf()\n",
    "        print(f\"Downloaded and converted to pandas HDF: {savepath}\")\n",
    "        \n",
    "    def convert_to_hdf(self):\n",
    "        label = IndexLabel(self.local_label_path)\n",
    "        df = label.read_index_data()\n",
    "        df.to_hdf(self.local_hdf_path, \"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-greene",
   "metadata": {},
   "source": [
    "## IndexDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export      \n",
    "\n",
    "class IndexDB:\n",
    "    fname = \"pds_indices_db.toml\"\n",
    "    fpath = Path.home() / f\".{fname}\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize index database.\n",
    "\n",
    "        Will copy the package's version to user's home folder at init,\n",
    "        so that user doesn't need to edit file in package to add new indices.\n",
    "\n",
    "        Adding new index URLs to the package's config file pds_indices_db.toml\n",
    "        is highly encouraged via pull request.\n",
    "        \"\"\"\n",
    "        if not self.fpath.exists():\n",
    "            with resource_path(\"planetarypy.pdstools.data\", self.fname) as p:\n",
    "                self.config = self.read_from_file(p)\n",
    "        else:\n",
    "            self.config = self.read_from_file()\n",
    "\n",
    "    def read_from_file(self, path=None):\n",
    "        \"\"\"Read the config.\n",
    "\n",
    "        Writing this short method to decouple IndexDB from choice of config file format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, pathlib.Path\n",
    "            Path to the config file to open.\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = self.fpath\n",
    "        return toml.load(path)\n",
    "\n",
    "    def write_to_file(self):\n",
    "        \"Write the config to user's home copy.\"\n",
    "        with open(self.fpath, \"w\") as f:\n",
    "            toml.dump(self.config, f)\n",
    "\n",
    "    def get_by_path(self, nested_key):\n",
    "        \"\"\"Get sub-dictionary by nested key.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nested_key: str\n",
    "            A nested key in the toml format, separated by '.', e.g. cassini.uvis.ring_summary\n",
    "        \"\"\"\n",
    "        mapList = nested_key.split(\".\")\n",
    "        d = copy.deepcopy(self.config)\n",
    "        for k in mapList:\n",
    "            d = d[k]\n",
    "        return Index(nested_key, **d)\n",
    "\n",
    "    def get(self, nested_key):\n",
    "        \"alias to get_by_path\"\n",
    "        return self.get_by_path(nested_key)\n",
    "\n",
    "    def set_by_path(self, nested_key, value):\n",
    "        \"\"\"Set a nested dictionary key to new value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nested_key : str\n",
    "            A nested key in the toml format, separated by '.', e.g. cassini.uvis.summary\n",
    "        value : str\n",
    "            New value for dictionary key\n",
    "        \"\"\"\n",
    "        dic = self.config\n",
    "        keys = nested_key.split(\".\")\n",
    "        for key in keys[:-1]:\n",
    "            dic = dic.setdefault(key, {})\n",
    "        dic[keys[-1]] = value\n",
    "\n",
    "    def list_indices(self):\n",
    "        \"Print index database in pretty form, using toml.dumps\"\n",
    "        print(toml.dumps(self.config))\n",
    "        print(\n",
    "            \"Use indices.download('mission.instrument.index') to download in index file.\"\n",
    "        )\n",
    "        print(\"For example: indices.download('cassini.uvis.moon_summary'\")\n",
    "\n",
    "    def update_timestamp(self, index):\n",
    "        self.set_by_path(f\"{index.key}.timestamp\", index.new_timestamp.isoformat())\n",
    "        self.write_to_file()\n",
    "\n",
    "    def download(\n",
    "        self, key=None, label_url=None, local_dir=\"\", convert_to_hdf=True, force=False\n",
    "    ):\n",
    "        \"\"\"Wrapping URLs for downloading PDS indices and their label files.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key : str, optional\n",
    "            Period-separated key into the available index files, e.g. cassini.uvis.moon_summary\n",
    "        label_url : str, optional\n",
    "            Alternative to using the index system, the user can provide the URL to a label\n",
    "            for an index. The table file has to be in the same folder, as usual.\n",
    "        local_dir: str, pathlib.Path, optional\n",
    "            Path for local storage. Default: current directory and filename from URL\n",
    "        convert_to_hdf : bool\n",
    "            Switch to convert the index automatically to a faster loading HDF file\n",
    "        \"\"\"\n",
    "        if label_url is None:\n",
    "            if key is not None:\n",
    "                index = self.get_by_path(key)\n",
    "            else:\n",
    "                raise SyntaxError(\"One of key or label_url needs to be given.\")\n",
    "        # check timestamp\n",
    "        if not index.needs_download and not force:\n",
    "            print(\"Stored index is up-to-date.\")\n",
    "            return index.local_hdf_path\n",
    "        if not local_dir:\n",
    "            local_dir = index.local_dir\n",
    "        label_url = index.url\n",
    "        logger.info(\"Downloading %s.\" % label_url)\n",
    "        local_label_path, _ = utils.download(label_url, local_dir)\n",
    "        data_url = replace_url_suffix(label_url)\n",
    "        logger.info(\"Downloading %s.\", data_url)\n",
    "        local_data_path, _ = utils.download(data_url, local_dir)\n",
    "        self.update_timestamp(index)\n",
    "        if convert_to_hdf is True:\n",
    "            label = IndexLabel(local_label_path)\n",
    "            df = label.read_index_data()\n",
    "            savepath = local_data_path.with_suffix(\".hdf\")\n",
    "            df.to_hdf(savepath, \"df\")\n",
    "        print(f\"Downloaded and converted to pandas HDF: {savepath}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return toml.dumps(self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "indexdb = IndexDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def list_available_index_files():\n",
    "    print(indexdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_url_suffix(url, new_suffix=\".tab\"):\n",
    "    \"\"\"Cleanest way to replace the suffix in an URL.\n",
    "\n",
    "    Sometimes the indices have upper case filenames, this is taken care of here.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URl to a file that has a suffix like .lbl\n",
    "    new_suffix : str, optional\n",
    "        The new suffix. Default (all cases so far): .img\n",
    "    \"\"\"\n",
    "    split = urlsplit(url)\n",
    "    old_suffix = Path(split.path).suffix\n",
    "    new_suffix = new_suffix.upper() if old_suffix.isupper() else new_suffix\n",
    "    return urlunsplit(\n",
    "        split._replace(path=str(Path(split.path).with_suffix(new_suffix)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-motorcycle",
   "metadata": {},
   "source": [
    "## PVLColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PVLColumn(object):\n",
    "    def __init__(self, pvlobj):\n",
    "        self.pvlobj = pvlobj\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.pvlobj[\"NAME\"]\n",
    "\n",
    "    @property\n",
    "    def name_as_list(self):\n",
    "        \"needs to return a list for consistency for cases when it's an array.\"\n",
    "        if self.items is None:\n",
    "            return [self.name]\n",
    "        else:\n",
    "            return [self.name + \"_\" + str(i + 1) for i in range(self.items)]\n",
    "\n",
    "    @property\n",
    "    def start(self):\n",
    "        \"Decrease by one as Python is 0-indexed.\"\n",
    "        return self.pvlobj[\"START_BYTE\"] - 1\n",
    "\n",
    "    @property\n",
    "    def stop(self):\n",
    "        return self.start + self.pvlobj[\"BYTES\"]\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.pvlobj.get(\"ITEMS\")\n",
    "\n",
    "    @property\n",
    "    def item_bytes(self):\n",
    "        return self.pvlobj.get(\"ITEM_BYTES\")\n",
    "\n",
    "    @property\n",
    "    def item_offset(self):\n",
    "        return self.pvlobj.get(\"ITEM_OFFSET\")\n",
    "\n",
    "    @property\n",
    "    def colspecs(self):\n",
    "        if self.items is None:\n",
    "            return (self.start, self.stop)\n",
    "        else:\n",
    "            i = 0\n",
    "            bucket = []\n",
    "            for _ in range(self.items):\n",
    "                off = self.start + self.item_offset * i\n",
    "                bucket.append((off, off + self.item_bytes))\n",
    "                i += 1\n",
    "            return bucket\n",
    "\n",
    "    def decode(self, linedata):\n",
    "        if self.items is None:\n",
    "            start, stop = self.colspecs\n",
    "            return linedata[start:stop]\n",
    "        else:\n",
    "            bucket = []\n",
    "            for (start, stop) in self.colspecs:\n",
    "                bucket.append(linedata[start:stop])\n",
    "            return bucket\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.pvlobj.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IndexLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IndexLabel(object):\n",
    "    \"\"\"Support working with label files of PDS Index tables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labelpath : str, pathlib.Path\n",
    "        Path to the labelfile for a PDS Indexfile. The actual table should reside in the same\n",
    "        folder to be automatically parsed when calling the `read_index_data` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labelpath):\n",
    "        self.path = Path(labelpath)\n",
    "        \"search for table name pointer and store key and fpath.\"\n",
    "        tuple = [i for i in self.pvl_lbl if i[0].startswith(\"^\")][0]\n",
    "        self.tablename = tuple[0][1:]\n",
    "        self.index_name = tuple[1]\n",
    "\n",
    "    @property\n",
    "    def index_path(self):\n",
    "        return self.path.parent / self.index_name\n",
    "\n",
    "    @property\n",
    "    def pvl_lbl(self):\n",
    "        return pvl.load(str(self.path))\n",
    "\n",
    "    @property\n",
    "    def table(self):\n",
    "        return self.pvl_lbl[self.tablename]\n",
    "\n",
    "    @property\n",
    "    def pvl_columns(self):\n",
    "        return self.table.getlist(\"COLUMN\")\n",
    "\n",
    "    @property\n",
    "    def columns_dic(self):\n",
    "        return {col[\"NAME\"]: col for col in self.pvl_columns}\n",
    "\n",
    "    @property\n",
    "    def colnames(self):\n",
    "        \"\"\"Read the columns in a ISS index label file.\n",
    "\n",
    "        The label file for the ISS indices describes the content\n",
    "        of the index files.\n",
    "        \"\"\"\n",
    "        colnames = []\n",
    "        for col in self.pvl_columns:\n",
    "            colnames.extend(PVLColumn(col).name_as_list)\n",
    "        return colnames\n",
    "\n",
    "    @property\n",
    "    def colspecs(self):\n",
    "        colspecs = []\n",
    "        columns = self.table.getlist(\"COLUMN\")\n",
    "        for column in columns:\n",
    "            pvlcol = PVLColumn(column)\n",
    "            if pvlcol.items is None:\n",
    "                colspecs.append(pvlcol.colspecs)\n",
    "            else:\n",
    "                colspecs.extend(pvlcol.colspecs)\n",
    "        return colspecs\n",
    "\n",
    "    def read_index_data(self, convert_times=True):\n",
    "        return index_to_df(self.index_path, self, convert_times=convert_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-future",
   "metadata": {},
   "source": [
    "### index_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def index_to_df(indexpath, label, convert_times=True):\n",
    "    \"\"\"The main reader function for PDS Indexfiles.\n",
    "\n",
    "    In conjunction with an IndexLabel object that figures out the column widths,\n",
    "    this reader should work for all PDS TAB files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indexpath : str or pathlib.Path\n",
    "        The path to the index TAB file.\n",
    "    label : pdstools.IndexLabel object\n",
    "        Label object that has both the column names and the columns widths as attributes\n",
    "        'colnames' and 'colspecs'\n",
    "    convert_times : bool\n",
    "        Switch to control if to convert columns with \"TIME\" in name (unless COUNT is as well in name) to datetime\n",
    "    \"\"\"\n",
    "    indexpath = Path(indexpath)\n",
    "    df = pd.read_fwf(\n",
    "        indexpath, header=None, names=label.colnames, colspecs=label.colspecs\n",
    "    )\n",
    "    if convert_times:\n",
    "        for column in [i for i in df.columns if \"TIME\" in i and \"COUNT\" not in i]:\n",
    "            if column == \"LOCAL_TIME\":\n",
    "                # don't convert local time\n",
    "                continue\n",
    "            print(f\"Converting times for column {column}.\")\n",
    "            try:\n",
    "                df[column] = pd.to_datetime(df[column])\n",
    "            except ValueError:\n",
    "                df[column] = pd.to_datetime(\n",
    "                    df[column], format=utils.nasa_dt_format_with_ms, errors=\"coerce\"\n",
    "                )\n",
    "        print(\"Done.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-component",
   "metadata": {},
   "source": [
    "### decode_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def decode_line(linedata, labelpath):\n",
    "    \"\"\"Decode one line of tabbed data with the appropriate label file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    linedata : str\n",
    "        One line of a .tab data file\n",
    "    labelpath : str or pathlib.Path\n",
    "        Path to the appropriate label that describes the data.\n",
    "    \"\"\"\n",
    "    label = IndexLabel(labelpath)\n",
    "    for column in label.pvl_columns:\n",
    "        pvlcol = PVLColumn(column)\n",
    "        print(pvlcol.name, pvlcol.decode(linedata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-adjustment",
   "metadata": {},
   "source": [
    "### find_mixed_type_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_mixed_type_cols(df, fix=True):\n",
    "    \"\"\"For a given dataframe, find the columns that are of mixed type.\n",
    "\n",
    "    Tool to help with the performance warning when trying to save a pandas DataFrame as a HDF.\n",
    "    When a column changes datatype somewhere, pickling occurs, slowing down the reading process of the HDF file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe to be searched for mixed data-types\n",
    "    fix : bool\n",
    "        Switch to control if NaN values in these problem columns should be replaced by the string 'UNKNOWN'\n",
    "    Returns\n",
    "    -------\n",
    "    List of column names that have data type changes within themselves.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for col in df.columns:\n",
    "        weird = (df[[col]].applymap(type) != df[[col]].iloc[0].apply(type)).any(axis=1)\n",
    "        if len(df[weird]) > 0:\n",
    "            print(col)\n",
    "            result.append(col)\n",
    "    if fix is True:\n",
    "        for col in result:\n",
    "            df[col].fillna(\"UNKNOWN\", inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-grain",
   "metadata": {},
   "source": [
    "### fix_hirise_edrcumindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fix_hirise_edrcumindex(infname, outfname):\n",
    "    \"\"\"Fix HiRISE EDRCUMINDEX.\n",
    "\n",
    "    The HiRISE EDRCUMINDEX has some broken lines where the SCAN_EXPOSURE_DURATION is of format\n",
    "    F10.4 instead of the defined F9.4.\n",
    "    This function simply replaces those incidences with one less decimal fraction, so 20000.0000\n",
    "    becomes 20000.000.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    infname : str\n",
    "        Path to broken EDRCUMINDEX.TAB\n",
    "    outfname : str\n",
    "        Path where to store the fixed TAB file\n",
    "    \"\"\"\n",
    "    with open(infname) as f:\n",
    "        with open(outfname, \"w\") as newf:\n",
    "            for line in tqdm(f):\n",
    "                exp = line.split(\",\")[21]\n",
    "                if float(exp) > 9999.999:\n",
    "                    # catching the return of write into dummy variable\n",
    "                    _ = newf.write(line.replace(exp, exp[:9]))\n",
    "                else:\n",
    "                    _ = newf.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'cassini.iss.index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(key='cassini.iss.index', url='https://pds-rings.seti.org/holdings/metadata/COISS_2xxx/COISS_2999/COISS_2999_index.lbl', timestamp='2019-11-19T23:37:22')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = indexdb.get(key)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-relationship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.needs_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-roulette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting times for column EARTH_RECEIVED_START_TIME.\n",
      "Converting times for column EARTH_RECEIVED_STOP_TIME.\n",
      "Converting times for column IMAGE_MID_TIME.\n",
      "Converting times for column IMAGE_TIME.\n",
      "Converting times for column PRODUCT_CREATION_TIME.\n",
      "Converting times for column START_TIME.\n",
      "Converting times for column STOP_TIME.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maye/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/generic.py:2605: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['FILE_NAME', 'FILE_SPECIFICATION_NAME', 'VOLUME_ID',\n",
      "       'ANTIBLOOMING_STATE_FLAG', 'CALIBRATION_LAMP_STATE_FLAG',\n",
      "       'COMMAND_FILE_NAME', 'DATA_CONVERSION_TYPE', 'DATA_SET_ID',\n",
      "       'DELAYED_READOUT_FLAG', 'DESCRIPTION', 'FILTER_NAME_1', 'FILTER_NAME_2',\n",
      "       'GAIN_MODE_ID', 'IMAGE_OBSERVATION_TYPE', 'INSTRUMENT_HOST_NAME',\n",
      "       'INSTRUMENT_ID', 'INSTRUMENT_MODE_ID', 'INSTRUMENT_NAME',\n",
      "       'INST_CMPRS_TYPE', 'LIGHT_FLOOD_STATE_FLAG', 'METHOD_DESC',\n",
      "       'MISSING_PACKET_FLAG', 'MISSION_NAME', 'MISSION_PHASE_NAME',\n",
      "       'OBSERVATION_ID', 'PRODUCT_ID', 'PRODUCT_VERSION_TYPE', 'SEQUENCE_ID',\n",
      "       'SEQUENCE_TITLE', 'SHUTTER_MODE_ID', 'SHUTTER_STATE_ID',\n",
      "       'SOFTWARE_VERSION_ID', 'TARGET_DESC', 'TARGET_NAME',\n",
      "       'TELEMETRY_FORMAT_ID', 'COORDINATE_SYSTEM_NAME', 'RINGS_FLAG',\n",
      "       'SPICE_PRODUCT_ID', 'TARGET_LIST', 'DATA_SET_NAME',\n",
      "       'INSTRUMENT_HOST_ID', 'PRODUCT_TYPE', 'STANDARD_DATA_PRODUCT_ID'],\n",
      "      dtype='object')]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "index.convert_to_hdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = index.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-aruba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 407299 entries, 0 to 407298\n",
      "Columns: 140 entries, FILE_NAME to STANDARD_DATA_PRODUCT_ID\n",
      "dtypes: datetime64[ns](7), float64(70), int64(20), object(43)\n",
      "memory usage: 438.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALIBRATION_LAMP_STATE_FLAG\n",
      "DESCRIPTION\n",
      "METHOD_DESC\n",
      "TARGET_LIST\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CALIBRATION_LAMP_STATE_FLAG', 'DESCRIPTION', 'METHOD_DESC', 'TARGET_LIST']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_mixed_type_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-acting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 407299 entries, 0 to 407298\n",
      "Columns: 140 entries, FILE_NAME to STANDARD_DATA_PRODUCT_ID\n",
      "dtypes: Float64(70), Int64(20), datetime64[ns](7), string(43)\n",
      "memory usage: 473.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.VOLUME_ID = df.VOLUME_ID.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-participation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 407299 entries, 0 to 407298\n",
      "Columns: 140 entries, FILE_NAME to STANDARD_DATA_PRODUCT_ID\n",
      "dtypes: Float64(70), Int64(20), category(1), datetime64[ns](7), string(42)\n",
      "memory usage: 470.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-wages",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FILE_NAME                     string\n",
       "FILE_SPECIFICATION_NAME       string\n",
       "VOLUME_ID                   category\n",
       "ANTIBLOOMING_STATE_FLAG       string\n",
       "BIAS_STRIP_MEAN              Float64\n",
       "                              ...   \n",
       "UPPER_RIGHT_LONGITUDE        Float64\n",
       "DATA_SET_NAME                 string\n",
       "INSTRUMENT_HOST_ID            string\n",
       "PRODUCT_TYPE                  string\n",
       "STANDARD_DATA_PRODUCT_ID      string\n",
       "Length: 140, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ANTIBLOOMING_STATE_FLAG = df.ANTIBLOOMING_STATE_FLAG.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-seventh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64Dtype()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.EMISSION_ANGLE.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-style",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-domain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cassini.iss.index]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COISS_2xxx/COISS_2999/COISS_2999_index.lbl\"\n",
      "timestamp = \"2019-11-19T23:37:22\"\n",
      "\n",
      "[cassini.iss.inventory]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COISS_2xxx/COISS_2999/COISS_2999_inventory.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[cassini.iss.moon_summary]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COISS_2xxx/COISS_2999/COISS_2999_moon_summary.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[cassini.iss.ring_summary]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COISS_2xxx/COISS_2999/COISS_2999_ring_summary.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[cassini.iss.saturn_summary]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COISS_2xxx/COISS_2999/COISS_2999_saturn_summary.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[cassini.uvis.index]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COUVIS_0xxx/COUVIS_0999/COUVIS_0999_index.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[cassini.uvis.moon_summary]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COUVIS_0xxx/COUVIS_0999/COUVIS_0999_moon_summary.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[cassini.uvis.ring_summary]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COUVIS_0xxx/COUVIS_0999/COUVIS_0999_ring_summary.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[cassini.uvis.saturn_summary]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COUVIS_0xxx/COUVIS_0999/COUVIS_0999_saturn_summary.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[cassini.uvis.supplemental_index]\n",
      "url = \"https://pds-rings.seti.org/holdings/metadata/COUVIS_0xxx/COUVIS_0999/COUVIS_0999_supplemental_index.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[mro.hirise.dtm]\n",
      "url = \"https://hirise-pds.lpl.arizona.edu/PDS/INDEX/DTMCUMINDEX.LBL\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[mro.hirise.edr]\n",
      "url = \"https://hirise-pds.lpl.arizona.edu/PDS/INDEX/EDRCUMINDEX.LBL\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[mro.hirise.rdr]\n",
      "url = \"https://hirise-pds.lpl.arizona.edu/PDS/INDEX/RDRCUMINDEX.LBL\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[lro.diviner.edr1]\n",
      "url = \"https://pds-geosciences.wustl.edu/lro/lro-l-dlre-2-edr-v1/lrodlr_0001/index/index.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[lro.diviner.edr2]\n",
      "url = \"https://pds-geosciences.wustl.edu/lro/lro-l-dlre-2-edr-v1/lrodlr_0002/index/index.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[lro.diviner.rdr1]\n",
      "url = \"https://pds-geosciences.wustl.edu/lro/lro-l-dlre-4-rdr-v1/lrodlr_1001/index/rdrindex.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[lro.diviner.rdr2]\n",
      "url = \"https://pds-geosciences.wustl.edu/lro/lro-l-dlre-4-rdr-v1/lrodlr_1002/index/rdrindex.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[lro.lola.edr]\n",
      "url = \"http://pds-geosciences.wustl.edu/lro/lro-l-lola-2-edr-v1/lrolol_0xxx/index/edrindex.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "[lro.lola.rdr]\n",
      "url = \"http://pds-geosciences.wustl.edu/lro/lro-l-lola-3-rdr-v1/lrolol_1xxx/index/rdrindex.lbl\"\n",
      "timestamp = \"\"\n",
      "\n",
      "Use indices.download('mission.instrument.index') to download in index file.\n",
      "For example: indices.download('cassini.uvis.moon_summary'\n"
     ]
    }
   ],
   "source": [
    "indexdb.list_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
