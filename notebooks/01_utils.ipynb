{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-heavy",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> General utilities. Should probably split up into `utils.time` and `utils.download`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab2c31-5d33-41ea-aae4-8d2d71483d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbverbose.showdoc import show_doc  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import datetime as dt\n",
    "import email.utils as eut\n",
    "import http.client as httplib\n",
    "import logging\n",
    "from typing import Union, Tuple\n",
    "from math import radians, tan\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    from osgeo import gdal\n",
    "except ImportError:\n",
    "    GDAL_INSTALLED = False\n",
    "    logger.warning(\n",
    "        \"No GDAL found. Some planetary.utils functions not working, but okay.\"\n",
    "    )\n",
    "else:\n",
    "    GDAL_INSTALLED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-module",
   "metadata": {},
   "source": [
    "## Time format strings\n",
    "\n",
    "These are the different format strings these utils convert from and to.\n",
    "\n",
    "An identifier with `xxx_dt_format_xxx` in its name signifies a full `datetime` format as compared to dates only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "nasa_date_format = \"%Y-%j\"\n",
    "nasa_dt_format = nasa_date_format + \"T%H:%M:%S\"\n",
    "nasa_dt_format_with_ms = nasa_dt_format + \".%f\"\n",
    "standard_date_format = \"%Y-%m-%d\"\n",
    "standard_dt_format = standard_date_format + \"T%H:%M:%S\"\n",
    "standard_dt_format_with_ms = standard_dt_format + \".%f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def nasa_date_to_iso(\n",
    "    datestr: str,  # Date string of the form Y-j\n",
    "    with_hours: bool = False,  # Switch if return is wanted with hours (i.e. isoformat)\n",
    ") -> str:  # Datestring in either Y-m-d or ISO-format.\n",
    "    \"Convert the day-number based NASA date format to ISO.\"\n",
    "    date = dt.datetime.strptime(datestr, nasa_date_format)\n",
    "    if with_hours:\n",
    "        return date.isoformat()\n",
    "    else:\n",
    "        return date.strftime(standard_date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_date = \"2010-110\"\n",
    "iso_date = \"2010-4-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nasa_date_to_iso(nasa_date, with_hours=True) == \"2010-04-20T00:00:00\"\n",
    "assert nasa_date_to_iso(nasa_date) == \"2010-04-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def iso_to_nasa_date(\n",
    "    datestr: str,  # Date string of the form Y-m-d\n",
    ") -> str:  # Datestring in NASA standard yyyy-jjj\n",
    "    \"Convert iso date to day-number based NASA date.\"\n",
    "    date = dt.datetime.strptime(datestr, standard_date_format)\n",
    "    return date.strftime(nasa_date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert iso_to_nasa_date(iso_date) == nasa_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def nasa_datetime_to_iso(\n",
    "    dtimestr: str,  # Datetime string of the form Y-jTH-M-S\n",
    "):  # Datestring in ISO standard yyyy-mm-ddTHH:MM:SS.xxxxxx\n",
    "    \"\"\"Convert the day-number based NASA datetime format to ISO.\n",
    "\n",
    "    Note: This is dateTIME vs `nasa_date_to_iso` which is just DATE.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dtimestr.split(\".\")[1]\n",
    "    except IndexError:\n",
    "        source_format = nasa_dt_format\n",
    "    else:\n",
    "        source_format = nasa_dt_format_with_ms\n",
    "    time = dt.datetime.strptime(dtimestr, source_format)\n",
    "    return time.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_datetime = \"2010-110T10:12:14\"\n",
    "nasa_datetime_with_ms = nasa_datetime + \".123000\"\n",
    "iso_datetime = \"2010-04-20T10:12:14\"\n",
    "iso_datetime_with_ms = iso_datetime + \".123000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nasa_datetime_to_iso(nasa_datetime) == iso_datetime\n",
    "assert nasa_datetime_to_iso(nasa_datetime_with_ms) == iso_datetime_with_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def iso_to_nasa_datetime(\n",
    "    dtimestr: str,  # Datetime string of the form yyyy-mm-ddTHH-MM-SS\n",
    "):  # Datestring in NASA standard yyyy-jjjTHH-MM-SS\n",
    "    \"Convert iso datetime to day-number based NASA datetime.\"\n",
    "    try:\n",
    "        dtimestr.split(\".\")[1]\n",
    "    except IndexError:\n",
    "        source_format = standard_dt_format\n",
    "        target_format = nasa_dt_format\n",
    "    else:\n",
    "        source_format = standard_dt_format_with_ms\n",
    "        target_format = nasa_dt_format_with_ms\n",
    "    date = dt.datetime.strptime(dtimestr, source_format)\n",
    "    return date.strftime(target_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert iso_to_nasa_datetime(iso_datetime) == nasa_datetime\n",
    "assert iso_to_nasa_datetime(iso_datetime_with_ms) == nasa_datetime_with_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22bfd3-1774-4272-9cc8-548fb421e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def replace_all_nasa_times(\n",
    "    df: pd.DataFrame,  # DataFrame with NASA time columns\n",
    "):\n",
    "    \"\"\"Find all NASA times in dataframe and replace with ISO.\n",
    "\n",
    "    Changes will be implemented on incoming dataframe!\n",
    "\n",
    "    This will be done for all columns with the word TIME in the column name.\n",
    "    \"\"\"\n",
    "    for col in [col for col in df.columns if \"TIME\" in col]:\n",
    "        if \"T\" in df[col].iloc[0]:\n",
    "            df[col] = pd.to_datetime(df[col].map(nasa_datetime_to_iso))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc0ee8-7058-4acf-847b-9610fc50abc3",
   "metadata": {},
   "source": [
    "## Network utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c4edf-347b-46e6-8825-3bc279565235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parse_http_date(\n",
    "    text: str,  # datestring from urllib.request\n",
    ") -> dt.datetime:  # dt.datetime object from given datetime string\n",
    "    \"Parse date string retrieved via urllib.request.\"\n",
    "    return dt.datetime(*eut.parsedate(text)[:6])\n",
    "\n",
    "\n",
    "def get_remote_timestamp(\n",
    "    url: str,  # URL to check timestamp for\n",
    ") -> dt.datetime:\n",
    "    \"\"\"Get the timestamp of a remote file.\n",
    "\n",
    "    Useful for checking if there's an updated file available.\n",
    "    \"\"\"\n",
    "    with urlopen(url, timeout=10) as conn:\n",
    "        t = parse_http_date(conn.headers[\"last-modified\"])\n",
    "    return t\n",
    "\n",
    "\n",
    "def url_retrieve(\n",
    "    url: str,  # The URL to download\n",
    "    outfile: str,  # The path where to store the downloaded file.\n",
    "    # The size of the chunk for the request.iter_content call. Default: 128\n",
    "    chunk_size: int = 128,\n",
    "):\n",
    "    \"\"\"Improved urlretrieve with progressbar, timeout and chunker.\n",
    "\n",
    "    This downloader has built-in progress bar using tqdm and using the `requests`\n",
    "    package it improves standard `urllib` behavior by adding time-out capability.\n",
    "\n",
    "    I tested different chunk_sizes and most of the time 128 was actually fastest, YMMV.\n",
    "\n",
    "    Inspired by https://stackoverflow.com/a/61575758/680232\n",
    "    \"\"\"\n",
    "    R = requests.get(url, stream=True, allow_redirects=True)\n",
    "    if R.status_code != 200:\n",
    "        raise ConnectionError(f\"Could not download {url}\\nError code: {R.status_code}\")\n",
    "    with tqdm.wrapattr(\n",
    "        open(outfile, \"wb\"),\n",
    "        \"write\",\n",
    "        miniters=1,\n",
    "        total=int(R.headers.get(\"content-length\", 0)),\n",
    "        desc=str(Path(outfile).name) + \"\\n\",\n",
    "    ) as fd:\n",
    "        for chunk in R.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "\n",
    "\n",
    "def have_internet():\n",
    "    \"\"\"Fastest way to check for active internet connection.\n",
    "\n",
    "    From https://stackoverflow.com/a/29854274/680232\n",
    "    \"\"\"\n",
    "    conn = httplib.HTTPConnection(\"www.google.com\", timeout=5)\n",
    "    try:\n",
    "        conn.request(\"HEAD\", \"/\")\n",
    "        conn.close()\n",
    "        return True\n",
    "    except:\n",
    "        conn.close()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a764e2-0ed9-42f4-8e56-fe73801ddaa8",
   "metadata": {},
   "source": [
    "## Image processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed487b52-99f9-4448-b281-8e54fcec6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def height_from_shadow(\n",
    "    shadow_in_pixels: float,  # Measured length of shadow in pixels\n",
    "    sun_elev: float,  # Ange of sun over horizon in degrees\n",
    ") -> float:  # Height [meter]\n",
    "    \"\"\"Calculate height of an object from its shadow length.\n",
    "\n",
    "    Note, that your image might have been binned.\n",
    "    You need to correct `shadow_in_pixels` for that.\n",
    "    \"\"\"\n",
    "    return tan(radians(sun_elev)) * shadow_in_pixels\n",
    "\n",
    "\n",
    "def get_gdal_center_coords(\n",
    "    imgpath: Union[str, Path],  # Path to raster image that is readable by GDLA\n",
    ") -> Tuple[int, int]:  # center row/col coordinates.\n",
    "    \"\"\"Get center rows/cols pixel coordinate for GDAL-readable dataset.\n",
    "\n",
    "    Check CLI `gdalinfo --formats` to see all formats that GDAL can open.\n",
    "    \"\"\"\n",
    "    if not GDAL_INSTALLED:\n",
    "        logger.error(\"GDAL not installed. Returning\")\n",
    "        return\n",
    "    ds = gdal.Open(str(imgpath))\n",
    "    xmean = ds.RasterXSize // 2\n",
    "    ymean = ds.RasterYSize // 2\n",
    "    return xmean, ymean\n",
    "\n",
    "\n",
    "def file_variations(\n",
    "    filename: Union[str, Path],  # The original filename to use as a base.\n",
    "    extensions: list\n",
    ") -> list:  # list of Paths\n",
    "    \"\"\"Create a variation of file names.\n",
    "\n",
    "    Generate a list of variations on a filename by replacing the extension with\n",
    "    the provided list.\n",
    "\n",
    "    Adapted from T. Olsens `file_variations of the pysis module for using pathlib.\n",
    "    \"\"\"\n",
    "    return [Path(filename).with_suffix(extension) for extension in extensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2766e4-724c-4308-88cc-741bcae6f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'abc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e96b5a-6a79-416c-9314-aaa9dbc8bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = \".cub .cal.cub .map.cal.cub\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77119135-7450-4e69-acf7-297218a2052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('abc.cub'), Path('abc.cal.cub'), Path('abc.map.cal.cub')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_variations(fname, extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725282d-24c8-4742-8b60-6f4d268d4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(extensions) == len(file_variations(fname, extensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c8f9c-3f44-49a9-89c3-0b3e04584cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
