{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d3ff7-3e9c-4e13-8b51-9f17906bd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp spice.kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6ff6a-9ee9-4c86-a2e4-7bbdfa81b7a0",
   "metadata": {},
   "source": [
    "# SPICE Kernels\n",
    "> Tools to manage SPICE kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58529cf6-6afa-45fc-ad3b-b1d27dd7f68d",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Feature list for this module:\n",
    "\n",
    "* Receive the list of supported datasets for automatic retrieval of archived SPICE kernels\n",
    "  * The supported datasets are tabled here at NAIF: https://naif.jpl.nasa.gov/naif/data_archived.html\n",
    "* Receive the list of required SPICE kernels for a given mission and time range\n",
    "* Automatic download of kernels for a given mission and time range either into a given location or the `planetarypy` local archive.\n",
    "\n",
    "As always in `planetarypy` the general design philosophy is to first develop a management class to give the user full control over all the details, and then add easy-to-use function for the end-user that do the most frequently used things in one go. (See section \"User Functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528ec83-7716-45f2-8c19-1b02b9980812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "import zipfile\n",
    "from datetime import timedelta\n",
    "from io import BytesIO\n",
    "from itertools import repeat\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import spiceypy as spice\n",
    "from astropy.time import Time\n",
    "from dask.distributed import Client\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from yarl import URL\n",
    "\n",
    "import pandas as pd\n",
    "from fastcore.test import test_fail\n",
    "from fastcore.utils import store_attr\n",
    "from planetarypy.config import config\n",
    "from planetarypy.utils import nasa_time_to_iso, url_retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ffdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96077c8-c27e-47ce-8246-6120fd3bbf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "KERNEL_STORAGE = config.storage_root / \"spice_kernels\"\n",
    "KERNEL_STORAGE.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ab2e1-8af9-49e3-b28d-0648ca55e150",
   "metadata": {},
   "source": [
    "## Identifying and downloading kernel sets\n",
    "\n",
    "One repeating task for SPICE calculations is the identification and retrieval of all SPICE kernels for a mission for a given time interval.\n",
    "\n",
    "The folks at NAIF offer a \"Subset\" feature at their servers.\n",
    "Here we set up a table of the currently supported datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c03f8-2058-4eef-9753-fce1342295dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "dataset_ids = {\n",
    "    \"cassini\": \"co-s_j_e_v-spice-6-v1.0/cosp_1000\",\n",
    "    \"clementine\": \"clem1-l-spice-6-v1.0/clsp_1000\",\n",
    "    \"dart\": \"dart/dart_spice\",\n",
    "    \"dawn\": \"dawn-m_a-spice-6-v1.0/dawnsp_1000\",\n",
    "    \"di\": \"di-c-spice-6-v1.0/disp_1000\",\n",
    "    \"ds1\": \"ds1-a_c-spice-6-v1.0/ds1sp_1000\",\n",
    "    \"epoxi\": \"dif-c_e_x-spice-6-v1.0/epxsp_1000\",\n",
    "    \"em16\": \"em16/em16_spice\",\n",
    "    \"grail\": \"grail-l-spice-6-v1.0/grlsp_1000\",\n",
    "    \"hayabusa\": \"hay-a-spice-6-v1.0/haysp_1000\",\n",
    "    \"insight\": \"insight/insight_spice\",\n",
    "    \"juno\": \"jno-j_e_ss-spice-6-v1.0/jnosp_1000\",\n",
    "    \"ladee\": \"ladee/ladee_spice\",\n",
    "    \"lro\": \"lro-l-spice-6-v1.0/lrosp_1000\",\n",
    "    \"maven\": \"maven/maven_spice\",\n",
    "    \"opportunity\": \"mer1-m-spice-6-v1.0/mer1sp_1000\",\n",
    "    \"spirit\": \"mer2-m-spice-6-v1.0/mer2sp_1000\",\n",
    "    \"messenger\": \"mess-e_v_h-spice-6-v1.0/messsp_1000\",\n",
    "    \"mars2020\": \"mars2020/mars2020_spice\",\n",
    "    \"mex\": \"mex-e_m-spice-6-v2.0/mexsp_2000\",\n",
    "    \"mgs\": \"mgs-m-spice-6-v1.0/mgsp_1000\",\n",
    "    \"ody\": \"ody-m-spice-6-v1.0/odsp_1000\",\n",
    "    \"mro\": \"mro-m-spice-6-v1.0/mrosp_1000\",\n",
    "    \"msl\": \"msl-m-spice-6-v1.0/mslsp_1000\",\n",
    "    \"near\": \"near-a-spice-6-v1.0/nearsp_1000\",\n",
    "    \"nh\": \"nh-j_p_ss-spice-6-v1.0/nhsp_1000\",\n",
    "    \"orex\": \"orex/orex_spice\",\n",
    "    \"rosetta\": \"ro_rl-e_m_a_c-spice-6-v1.0/rossp_1000\",\n",
    "    \"stardust\": \"sdu-c-spice-6-v1.0/sdsp_1000\",\n",
    "    \"venus_climate_orbiter\": \"vco/vco_spice\",\n",
    "    \"vex\": \"vex-e_v-spice-6-v2.0/vexsp_2000\",\n",
    "    \"vo\": \"vo1_vo2-m-spice-6-v1.0/vosp_1000\",\n",
    "}\n",
    "\n",
    "df = pd.DataFrame({\"shorthand\": dataset_ids.keys(), \"path\": dataset_ids.values()})\n",
    "\n",
    "df2 = pd.read_html(\"https://naif.jpl.nasa.gov/naif/data_archived.html\")[6]\n",
    "df2.columns = df2.iloc[0]\n",
    "df2 = df2.drop(0).reset_index(drop=True)\n",
    "df2 = df2.drop([\"Archive Readme\", \"Archive Link\", \"Subset Link\"], axis=1)\n",
    "df = df.join(df2)\n",
    "datasets = df.set_index(\"shorthand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9eb278-d5b7-4aa8-9452-a74461045178",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce4e7e-9e95-4ce4-831f-8595e4b7c5f7",
   "metadata": {},
   "source": [
    "To receive this dataframe:\n",
    "\n",
    "```python\n",
    "from planetarypy.spice.kernels import datasets\n",
    "```\n",
    "Some validation helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8c2c8-15bd-44c7-9ffc-b75600efeb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_start_valid(\n",
    "        mission: str,  # mission shorthand label of datasets dataframe\n",
    "        start: Time,  # start time in astropy.Time format\n",
    "):\n",
    "    return Time(datasets.at[mission, \"Start Time\"]) <= start\n",
    "\n",
    "\n",
    "def is_stop_valid(\n",
    "        mission: str,  # mission shorthand label of datasets dataframe\n",
    "        stop: Time,  # stop time in astropy.Time format\n",
    "):\n",
    "    return Time(datasets.at[mission, \"Stop Time\"]) >= stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03347e-6094-4ff4-9d6b-19ca639ea5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_start_valid(\"cassini\", Time(\"1998-01-01\")) is True\n",
    "assert is_start_valid(\"cassini\", Time(\"1997-01-01\")) is False\n",
    "assert is_stop_valid(\"cassini\", \"2017-01-01\") is True\n",
    "assert is_stop_valid(\"cassini\", \"2018-01-01\") is False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab6332-befc-458e-a8fc-252021a7d4f8",
   "metadata": {},
   "source": [
    "Now we build a management class for wrapping the Perl script available at below's URL for accessing subsets of these datasets.\n",
    "\n",
    "First, the basic URLs we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558e311-d97c-42e0-a59b-4bc7172dad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "NAIF_URL = URL(\"https://naif.jpl.nasa.gov\")\n",
    "BASE_URL = NAIF_URL / \"cgi-bin/subsetds.pl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a6f9b4-2473-4221-b222-672db76ad019",
   "metadata": {},
   "source": [
    "The Perl script `subsetds.pl` (the name at the end of the `BASE_URL`) requires as input:\n",
    "\n",
    "* the dataset name\n",
    "* start and stop of the time interval\n",
    "* a constant named \"Subset\" to identify the action for this Perl script\n",
    "\n",
    "We can assemble these parameters into a payload dictionary for the `requests.get` call and we manage different potential actions on the zipfile with a `Subsetter` class, that only requires the mission identifier, start and stop as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f412af2-7ed7-48bd-bfcd-f4f17427e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def download_one_url(url, local_path, overwrite: bool = False):\n",
    "    if local_path.exists() and not overwrite:\n",
    "        return\n",
    "    local_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    url_retrieve(url, local_path)\n",
    "\n",
    "\n",
    "class Subsetter:\n",
    "    \"\"\"Class to manage retrieving subset SPICE kernel lists\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    kernel_names: List of\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mission: str,  # mission shorthand in datasets dataframe\n",
    "            start: str,  # start time in either ISO or yyyy-jjj format\n",
    "            stop=None,  # stop time in either ISO or yyyy-jjj format\n",
    "            save_location=None,  # overwrite default storing in planetarpy archive\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        r = self.r\n",
    "        if r.ok:\n",
    "            z = zipfile.ZipFile(BytesIO(r.content))\n",
    "        else:\n",
    "            raise IOError(\"SPICE Server request returned status code: {r.status_code}\")\n",
    "        self.z = z\n",
    "        # these files only exist \"virtually\" in the zip object, but are needed to\n",
    "        # extract them:\n",
    "        self.urls_file = [n for n in z.namelist() if n.startswith(\"urls_\")][0]\n",
    "        self.metakernel_file = [n for n in z.namelist() if n.lower().endswith(\".tm\")][0]\n",
    "        with self.z.open(self.urls_file) as f:\n",
    "            self.kernel_urls = f.read().decode().split()\n",
    "\n",
    "    @property\n",
    "    def r(self):\n",
    "        return requests.get(BASE_URL, params=self.payload, stream=True)\n",
    "\n",
    "    @property\n",
    "    def start(self):\n",
    "        return self._start\n",
    "\n",
    "    @start.setter\n",
    "    def start(self, value):\n",
    "        try:\n",
    "            self._start = Time(value)\n",
    "        except ValueError:\n",
    "            self._start = Time(nasa_time_to_iso(value))\n",
    "\n",
    "    @property\n",
    "    def stop(self):\n",
    "        return self._stop\n",
    "\n",
    "    @stop.setter\n",
    "    def stop(self, value):\n",
    "        if not value:\n",
    "            self._stop = self.start + timedelta(days=1)\n",
    "        else:\n",
    "            try:\n",
    "                self._stop = Time(value)\n",
    "            except ValueError:\n",
    "                self._stop = Time(nasa_time_to_iso(value))\n",
    "\n",
    "    @property\n",
    "    def payload(self):\n",
    "        \"\"\"Put payload together while checking for working time format.\n",
    "\n",
    "        If Time(self.start) doesn't work, then we assume that the date must be in the\n",
    "        Time-unsupported yyyy-jjj format, which can be converted by `nasa_time_to_iso`\n",
    "        from `planetarypy.utils`.\n",
    "        \"\"\"\n",
    "        if not (is_start_valid(self.mission, self.start) and is_stop_valid(self.mission, self.stop)):\n",
    "            raise ValueError(\"One of start/stop is outside the supported date-range. See `datasets`.\")\n",
    "        p = {\n",
    "            \"dataset\": dataset_ids[self.mission],\n",
    "            \"start\": self.start.iso,\n",
    "            \"stop\": self.stop.iso,\n",
    "            \"action\": \"Subset\",\n",
    "        }\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def kernel_names(self):\n",
    "        \"Return list of names of kernels for the given time range.\"\n",
    "        return [str(Path(URL(url).parent.name) / URL(url).name) for url in self.kernel_urls]\n",
    "\n",
    "    def get_local_path(\n",
    "            self,\n",
    "            url,  # kernel url to determine local storage path\n",
    "    ) -> Path:  # full local path where kernel in URL will be stored\n",
    "        \"\"\"Calculate local storage path from Kernel URL, using `save_location` if given.\n",
    "\n",
    "        If self.save_location is None, the `planetarypy` archive is being used.\n",
    "        \"\"\"\n",
    "        u = URL(url)\n",
    "        basepath = (KERNEL_STORAGE / self.mission if not self.save_location else self.save_location)\n",
    "        return basepath / u.parent.name / u.name\n",
    "\n",
    "    def _non_blocking_download(self, overwrite: bool = False):\n",
    "        with Client() as client:\n",
    "            futures = []\n",
    "            for url in tqdm(self.kernel_urls, desc=\"Kernels downloaded\"):\n",
    "                local_path = self.get_local_path(url)\n",
    "                if local_path.exists() and not overwrite:\n",
    "                    print(local_path.parent.name, local_path.name, \"locally available.\")\n",
    "                    continue\n",
    "                local_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "                futures.append(client.submit(url_retrieve, url, local_path))\n",
    "            return [f.result() for f in futures]\n",
    "\n",
    "    def _concurrent_download(self, overwrite: bool = False):\n",
    "        paths = [self.get_local_path(url) for url in self.kernel_urls]\n",
    "        args = zip(self.kernel_urls, paths, repeat(overwrite))\n",
    "        results = process_map(download_one_url, args, max_workers=cpu_count() - 2)\n",
    "\n",
    "    def download_kernels(\n",
    "        self,\n",
    "        overwrite: bool = False,  # switch to control if kernels should be downloaded over existing ones\n",
    "        non_blocking: bool = False,\n",
    "    ):\n",
    "        if non_blocking:\n",
    "            return self._non_blocking_download(overwrite)\n",
    "        # sequential download\n",
    "        for url in tqdm(self.kernel_urls, desc=\"Kernels downloaded\"):\n",
    "            local_path = self.get_local_path(url)\n",
    "            if local_path.exists() and not overwrite:\n",
    "                print(local_path.parent.name, local_path.name, \"locally available.\")\n",
    "                continue\n",
    "            local_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "            url_retrieve(url, local_path)\n",
    "\n",
    "    def get_metakernel(self) -> Path:  # return path to metakernel file\n",
    "        \"\"\"Get metakernel file from NAIF and adapt path to match local storage.\n",
    "\n",
    "        Use `save_location` if given, otherwise `planetarypy` archive.\n",
    "        \"\"\"\n",
    "        basepath = (KERNEL_STORAGE / self.mission if not self.save_location else self.save_location)\n",
    "        savepath = basepath / self.metakernel_file\n",
    "        with open(savepath, \"w\") as outfile, self.z.open(self.metakernel_file) as infile:\n",
    "            for line in infile:\n",
    "                linestr = line.decode()\n",
    "                if \"'./data'\" in linestr:\n",
    "                    linestr = linestr.replace(\"'./data'\", f\"'{savepath.parent}'\")\n",
    "                outfile.write(linestr)\n",
    "        return savepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289cefc-e6b7-45ae-aa82-1f880c332dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = Subsetter(\"cassini\", \"2014-270\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04a242-8144-4880-9bbd-9197a5f814e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.kernel_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b6827-9ec4-42b6-abb4-88ea13bfdcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should fail:\n",
    "def _failing():\n",
    "    Subsetter(\"cassini\", \"2019-01-01\")\n",
    "\n",
    "\n",
    "test_fail(_failing, contains=\"start/stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54fec7-ed64-4855-a23c-7fd522920f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = Subsetter(\"cassini\", \"2011-02-13\", \"2011-02-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291ffb3-2c03-44a3-8513-b03c1858bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.urls_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbe5f5-1757-41c2-8476-8218e8d6c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.metakernel_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eab68b-2f7f-4121-93e6-b01e8d36e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Subsetter.kernel_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2e0a4-0884-4438-a928-370b5a4fd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.kernel_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc24353-a837-45b9-b63e-0eed3e12fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.kernel_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182fde2-5feb-4bb4-8760-5b01d3b4d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Subsetter.get_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a42d38-f8cd-4b01-ad07-79725c0357f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.get_local_path(subset.kernel_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b0f92-5308-4282-bc0f-814d390c219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.save_location = Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e5703-c660-4ec7-b957-f549f7340245",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.get_local_path(subset.kernel_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ade41-6cae-4743-b893-69b416eacf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Subsetter.download_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac7b78-2561-47c5-8818-60ff5244ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset save_location to prevent additional download\n",
    "subset.save_location = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52c22a-234a-48e6-b811-a239dd9c33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.download_kernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044238d3-5931-4c0c-9ee9-3174ec7a4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.download_kernels(non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f26bae-42f5-43df-9a81-0fb70ac663a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Subsetter.get_metakernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d6ddd-50c8-4c9a-9592-9c1f1fee25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkpath = subset.get_metakernel()\n",
    "mkpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283507d5-581d-4741-acb5-2926e278e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {mkpath}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed329fb-9f18-471b-a0f6-4f8408048ad6",
   "metadata": {},
   "source": [
    "Loading the metakernel works! :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f908c-2d91-4dc5-b14d-57d724b6beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spice.furnsh(str(mkpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c69128-dfa9-4637-9352-b4f0d6c8f6b8",
   "metadata": {},
   "source": [
    "Or, with given `save_location`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad0619-5c38-4422-bc0d-5d6ddf53f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.save_location = Path(\".\")\n",
    "mkpath = subset.get_metakernel()\n",
    "mkpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f5a30-e29d-4051-ba0f-8b3236207fd7",
   "metadata": {},
   "source": [
    "The metakernel is correctly adapted, however for these tests, I didn't download the kernels again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834f517-eb7b-430f-92f1-c483ac3dc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {mkpath}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7487fc-1c1a-434d-bcf4-5f9ad3984a9c",
   "metadata": {},
   "source": [
    "## User functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08ace9-e577-4e81-bb6d-6bfa232f3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_metakernel_and_files(\n",
    "        mission: str,  # mission shorthand from datasets dataframe\n",
    "        start: str,  # start time as iso-string, or yyyy-jjj\n",
    "        stop: str,  # stop time as iso-string or yyyy-jjj\n",
    "        save_location: str = None,  # override storage into planetarypy archive\n",
    ") -> Path:  # pathlib.Path to metakernel file with corrected data path.\n",
    "    \"For a given mission and start/stop times, download the kernels and get metakernel path\"\n",
    "    subset = Subsetter(mission, start, stop, save_location)\n",
    "    subset.download_kernels(non_blocking=True)\n",
    "    return subset.get_metakernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68deb5da-5bd5-44d8-be01-edba72037ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkpath = get_metakernel_and_files(\"cassini\", \"2011-02-13\", \"2011-02-14\")\n",
    "mkpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7bd8b8-2a55-4386-9a98-e3c7c7f425ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_kernels_for_day(\n",
    "        mission: str,  # mission shorthand from datasets dataframe\n",
    "        start: str,  # start time as iso-string, or yyyy-jjj\n",
    "        stop: str = \"\",  # stop time as iso-string or yyyy-jjj\n",
    ") -> list:  # list of kernel names\n",
    "    subset = Subsetter(mission, start, stop)\n",
    "    return subset.kernel_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b05dd-3b42-4fa1-a738-bbddc1b01e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_kernels_for_day(\"mro\", \"2015-02-13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1026c5-2223-4244-904d-61510813a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_kernels_for_day(\"maven\", \"2017-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdece451-f66b-432f-95a5-6b5f5c39f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |filter_stream ErfaWarning\n",
    "\n",
    "\n",
    "def _test_mission_kernels_available(mission):\n",
    "    print(\"Doing\", mission)\n",
    "    start = datasets.at[mission, \"Start Time\"]\n",
    "    end = datasets.at[mission, \"Stop Time\"]\n",
    "    half = Time(start) + (Time(end) - Time(start)) / 2\n",
    "    print(\"Half time:\", half)\n",
    "    try:\n",
    "        found = list_kernels_for_day(mission, half)\n",
    "    except IndexError:\n",
    "        print(\"Problem with\", mission)\n",
    "    else:\n",
    "        print(f\"Found {len(found)} kernels for {mission}\")\n",
    "\n",
    "\n",
    "# futures = []\n",
    "# with Client() as client:\n",
    "# for mission in datasets.index:\n",
    "#     futures.append(client.submit(_test_mission_kernels_available, mission))\n",
    "# [f.result() for f in futures]\n",
    "for mission in datasets.index:\n",
    "    _test_mission_kernels_available(mission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12201c1-687c-422f-9717-4f528e667f78",
   "metadata": {},
   "source": [
    "> NOTE: Any ErfaWarnings above are caused by the LADEE mission using a kernel up to 2050, and the astropy.Time module warns about potential precicision issues regarding unknown leapseconds that will be put in in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e79890-df55-487a-91c8-8962f0c3f806",
   "metadata": {},
   "source": [
    "### Generic kernel management\n",
    "\n",
    "There are a few generic kernels that are required for basic illumination calculations as supported by this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71580983-4661-4e81-ae0e-a4c09b83538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "GENERIC_STORAGE = KERNEL_STORAGE / \"generic\"\n",
    "GENERIC_STORAGE.mkdir(exist_ok=True, parents=True)\n",
    "GENERIC_URL = NAIF_URL / \"pub/naif/generic_kernels/\"\n",
    "\n",
    "generic_kernel_names = [\n",
    "    \"lsk/naif0012.tls\",\n",
    "    \"pck/pck00010.tpc\",\n",
    "    \"pck/de-403-masses.tpc\",\n",
    "    \"spk/planets/de430.bsp\",\n",
    "    \"spk/satellites/mar097.bsp\",\n",
    "]\n",
    "generic_kernel_paths = [GENERIC_STORAGE.joinpath(i) for i in generic_kernel_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5deb59-19df-40ef-8b0d-b33264a906ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def download_generic_kernels(overwrite=False):\n",
    "    \"Download all kernels as required by generic_kernel_list.\"\n",
    "    dl_urls = [GENERIC_URL / i for i in generic_kernel_names]\n",
    "    for dl_url, savepath in zip(dl_urls, generic_kernel_paths):\n",
    "        if savepath.exists() and not overwrite:\n",
    "            print(savepath.name, \"already downloaded. Use `overwrite=True` to download again.\")\n",
    "            continue\n",
    "        savepath.parent.mkdir(exist_ok=True, parents=True)\n",
    "        url_retrieve(dl_url, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe088f9-33e3-4d43-bc24-a82846974d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_generic_kernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e71fd-f8e0-40a7-9c6f-49f8acbcefd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def load_generic_kernels():\n",
    "    \"\"\"Load all kernels in generic_kernels list.\n",
    "\n",
    "    Loads pure planetary bodies meta-kernel without spacecraft data.\n",
    "\n",
    "    Downloads any missing generic kernels.\n",
    "    \"\"\"\n",
    "    if any([not p.exists() for p in generic_kernel_paths]):\n",
    "        download_generic_kernels()\n",
    "    for kernel in generic_kernel_paths:\n",
    "        spice.furnsh(str(kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbe7d7-46ec-4106-b8ff-ea9a907d9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "spice.kclear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0171d-30ff-4ec9-b889-264825db654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_generic_kernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1a163-a5b6-4b96-b942-9d44acdb9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def show_loaded_kernels():\n",
    "    \"Print overview of loaded kernels.\"\n",
    "    count = spice.ktotal(\"all\")\n",
    "    if count == 0:\n",
    "        print(\"No kernels loaded at this time.\")\n",
    "    else:\n",
    "        print(\"The loaded files are:\\n(paths relative to kernels.KERNEL_STORAGE)\\n\")\n",
    "    for which in range(count):\n",
    "        out = spice.kdata(which, \"all\", 100, 100, 100)\n",
    "        print(\"Position:\", which)\n",
    "        p = Path(out[0])\n",
    "        print(\"Path\", p.relative_to(KERNEL_STORAGE))\n",
    "        print(\"Type:\", out[1])\n",
    "        print(\"Source:\", out[2])\n",
    "        print(\"Handle:\", out[3])\n",
    "        # print(\"Found:\", out[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb3c4c-cf5f-467d-9b2e-0e0abc4476dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loaded_kernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6bfbd-7bc6-4b0b-90ae-666f49b7f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-py39-py",
   "language": "python",
   "name": "conda-env-py39-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
