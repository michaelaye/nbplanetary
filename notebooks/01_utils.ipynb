{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-heavy",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> General utilities. Should probably split up into `utils.time` and `utils.download`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import datetime as dt\n",
    "import email.utils as eut\n",
    "import http.client as httplib\n",
    "import logging\n",
    "from math import radians, tan\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    from osgeo import gdal\n",
    "except ImportError:\n",
    "    GDAL_INSTALLED = False\n",
    "    logger.warning(\n",
    "        \"No GDAL found. Some planetary.utils functions not working, but okay.\"\n",
    "    )\n",
    "else:\n",
    "    GDAL_INSTALLED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-module",
   "metadata": {},
   "source": [
    "## Time format strings\n",
    "\n",
    "These are the different format strings these utils convert from and to.\n",
    "\n",
    "An identifier with `_dt_` in its name signifies a full `datetime` format as compared to dates only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "nasa_date_format = \"%Y-%j\"\n",
    "nasa_dt_format = nasa_date_format + \"T%H:%M:%S\"\n",
    "nasa_dt_format_with_ms = nasa_dt_format + \".%f\"\n",
    "standard_date_format = \"%Y-%m-%d\"\n",
    "standard_dt_format = standard_date_format + \"T%H:%M:%S\"\n",
    "standard_dt_format_with_ms = standard_dt_format + \".%f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def nasa_date_to_iso(datestr, with_hours=False):\n",
    "    \"\"\"Convert the day-number based NASA date format to ISO.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datestr : str\n",
    "        Date string in the form Y-j\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Datestring in ISO standard yyyy-mm-ddTHH:MM:SS.MMMMMM\n",
    "    \"\"\"\n",
    "    date = dt.datetime.strptime(datestr, nasa_date_format)\n",
    "    if with_hours:\n",
    "        return date.isoformat()\n",
    "    else:\n",
    "        return date.strftime(standard_date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_date = \"2010-110\"\n",
    "iso_date = \"2010-4-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nasa_date_to_iso(nasa_date, with_hours=True) == \"2010-04-20T00:00:00\"\n",
    "assert nasa_date_to_iso(nasa_date) == \"2010-04-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def iso_to_nasa_date(datestr):\n",
    "    \"\"\"Convert iso date to day-number based NASA date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datestr : str\n",
    "        Date string in the form Y-m-d\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Datestring in NASA standard yyyy-jjj\n",
    "    \"\"\"\n",
    "    date = dt.datetime.strptime(datestr, standard_date_format)\n",
    "    return date.strftime(nasa_date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert iso_to_nasa_date(iso_date) == nasa_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def nasa_datetime_to_iso(dtimestr):\n",
    "    \"\"\"Convert the day-number based NASA datetime format to ISO.\n",
    "\n",
    "    Note: This is dateTIME vs `nasa_date_to_iso` which is just DATE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dtimestr : str\n",
    "        Datetime string in the form Y-jTH:M:S\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Datestring in ISO standard yyyy-mm-ddTHH:MM:SS.MMMMMM\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dtimestr.split(\".\")[1]\n",
    "    except IndexError:\n",
    "        source_format = nasa_dt_format\n",
    "    else:\n",
    "        source_format = nasa_dt_format_with_ms\n",
    "    time = dt.datetime.strptime(dtimestr, source_format)\n",
    "    return time.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_datetime = \"2010-110T10:12:14\"\n",
    "nasa_datetime_with_ms = nasa_datetime + \".123000\"\n",
    "iso_datetime = \"2010-04-20T10:12:14\"\n",
    "iso_datetime_with_ms = iso_datetime + \".123000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nasa_datetime_to_iso(nasa_datetime) == iso_datetime\n",
    "assert nasa_datetime_to_iso(nasa_datetime_with_ms) == iso_datetime_with_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def iso_to_nasa_datetime(dtimestr):\n",
    "    \"\"\"Convert iso datetime to day-number based NASA datetime.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dtimestr : str\n",
    "        Datetime string in the form yyyy-mm-ddTHH-MM-SS\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Datestring in NASA standard yyyy-jjjTHH-MM-SS\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dtimestr.split(\".\")[1]\n",
    "    except IndexError:\n",
    "        source_format = standard_dt_format\n",
    "        target_format = nasa_dt_format\n",
    "    else:\n",
    "        source_format = standard_dt_format_with_ms\n",
    "        target_format = nasa_dt_format_with_ms\n",
    "    date = dt.datetime.strptime(dtimestr, source_format)\n",
    "    return date.strftime(target_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert iso_to_nasa_datetime(iso_datetime) == nasa_datetime\n",
    "assert iso_to_nasa_datetime(iso_datetime_with_ms) == nasa_datetime_with_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22bfd3-1774-4272-9cc8-548fb421e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def replace_all_nasa_times(df):\n",
    "    \"\"\"Find all NASA times in dataframe and replace with ISO.\n",
    "\n",
    "    Changes will be implemented on incoming dataframe!\n",
    "\n",
    "    This will be done for all columns with the word TIME in the column name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        DataFrame with NASA time columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Nothing (Changes implemented in-place)\n",
    "    \"\"\"\n",
    "    for col in [col for col in df.columns if \"TIME\" in col]:\n",
    "        if \"T\" in df[col].iloc[0]:\n",
    "            df[col] = pd.to_datetime(df[col].map(nasa_datetime_to_iso))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc0ee8-7058-4acf-847b-9610fc50abc3",
   "metadata": {},
   "source": [
    "## Network utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c4edf-347b-46e6-8825-3bc279565235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ProgressBar(tqdm):\n",
    "    \"\"\"Provides `update_to(n)` which uses `tqdm.update(delta_n)`.\"\"\"\n",
    "\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        \"\"\"\n",
    "        b  : int, optional\n",
    "            Number of blocks transferred so far [default: 1].\n",
    "        bsize  : int, optional\n",
    "            Size of each block (in tqdm units) [default: 1].\n",
    "        tsize  : int, optional\n",
    "            Total size (in tqdm units). If [default: None] remains unchanged.\n",
    "        \"\"\"\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)  # will also set self.n = b * bsize\n",
    "\n",
    "\n",
    "def parse_http_date(text):\n",
    "    \"Parse date string retrieved via urllib.request.\"\n",
    "    return dt.datetime(*eut.parsedate(text)[:6])\n",
    "\n",
    "\n",
    "def get_remote_timestamp(url):\n",
    "    conn = urlopen(url, timeout=10)\n",
    "    t = parse_http_date(conn.headers[\"last-modified\"])\n",
    "    conn.close()\n",
    "    return t\n",
    "\n",
    "\n",
    "def download(url, local_dir=\".\", use_tqdm=True, **kwargs):\n",
    "    \"\"\"Simple wrapper of urlretrieve\n",
    "\n",
    "    Adding a default path to urlretrieve\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    url : str\n",
    "        HTTP(S) URL to download\n",
    "    local_dir : str,pathlib.Path\n",
    "        Local directory where to store the download.\n",
    "    **kwargs : {dict}\n",
    "        Keyword args to be handed to urlretrieve.\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "        Tuple returned by urlretrieve\n",
    "    \"\"\"\n",
    "    name = url.split(\"/\")[-1]\n",
    "    local = Path(local_dir)\n",
    "    savepath = local / name if local.is_dir() else local\n",
    "    logger.debug(\"Downloading %s into %s\", url, savepath)\n",
    "    if use_tqdm:\n",
    "        with ProgressBar(\n",
    "            unit=\"B\", unit_scale=True, miniters=1, desc=url.split(\"/\")[-1]\n",
    "        ) as t:  # all optional kwargs\n",
    "            return urlretrieve(url, savepath, reporthook=t.update_to)\n",
    "    else:\n",
    "        return urlretrieve(url, savepath, **kwargs)\n",
    "\n",
    "\n",
    "def url_retrieve(url: str, outfile: str, chunk_size: int = 128):\n",
    "    \"\"\"Improved urlretrieve with progressbar, timeout and chunker.\n",
    "\n",
    "    This downloader has built-in progress bar using tqdm and using the `requests`\n",
    "    package it improves standard `urllib` behavior by adding time-out capability.\n",
    "\n",
    "    I tested different chunk_sizes and most of the time 128 was actually fastest, YMMV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str, urlpath.URL\n",
    "        The URL to download\n",
    "    outfile: str, pathlib.Path\n",
    "        The path where to store the downloaded file.\n",
    "    chunk_size : int, optional\n",
    "        The size of the chunk for the request.iter_content call. Default: 128\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    Inspired by https://stackoverflow.com/a/61575758/680232\n",
    "    \"\"\"\n",
    "    R = requests.get(url, stream=True, allow_redirects=True)\n",
    "    if R.status_code != 200:\n",
    "        raise ConnectionError(f\"Could not download {url}\\nError code: {R.status_code}\")\n",
    "    with tqdm.wrapattr(\n",
    "        open(outfile, \"wb\"),\n",
    "        \"write\",\n",
    "        miniters=1,\n",
    "        total=int(R.headers.get(\"content-length\", 0)),\n",
    "        desc=str(Path(outfile).name) + \"\\n\",\n",
    "    ) as fd:\n",
    "        for chunk in R.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "\n",
    "\n",
    "def have_internet():\n",
    "    conn = httplib.HTTPConnection(\"www.google.com\", timeout=5)\n",
    "    try:\n",
    "        conn.request(\"HEAD\", \"/\")\n",
    "        conn.close()\n",
    "        return True\n",
    "    except:\n",
    "        conn.close()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a764e2-0ed9-42f4-8e56-fe73801ddaa8",
   "metadata": {},
   "source": [
    "## Image processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed487b52-99f9-4448-b281-8e54fcec6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def height_from_shadow(shadow_in_pixels, sun_elev):\n",
    "    \"\"\"Calculate height of an object from its shadow length.\n",
    "\n",
    "    Note, that your image might have been binned.\n",
    "    You need to correct `shadow_in_pixels` for that.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shadow_in_pixels : float\n",
    "        Measured length of shadow in pixels\n",
    "    sun_elev : angle(float)\n",
    "        Angle of sun over horizon\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    height [meter]\n",
    "    \"\"\"\n",
    "    return tan(radians(sun_elev)) * shadow_in_pixels\n",
    "\n",
    "# export\n",
    "\n",
    "\n",
    "def get_gdal_center_coords(imgpath):\n",
    "    \"\"\"Get center rows/cols pixel coordinate for GDAL-readable dataset.\n",
    "\n",
    "    Check CLI `gdalinfo --formats` to see all formats that GDAL can open.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgpath: str, pathlib.Path\n",
    "        Path to raster image that is readable by GDLA\n",
    "    \"\"\"\n",
    "    if not GDAL_INSTALLED:\n",
    "        logger.error(\"GDAL not installed. Returning\")\n",
    "        return\n",
    "    ds = gdal.Open(str(imgpath))\n",
    "    xmean = ds.RasterXSize // 2\n",
    "    ymean = ds.RasterYSize // 2\n",
    "    return xmean, ymean\n",
    "\n",
    "\n",
    "def file_variations(filename, extensions):\n",
    "    \"\"\"Create a variation of file names.\n",
    "    \n",
    "    Generate a list of variations on a filename by replacing the extension with\n",
    "    the provided list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str, pathlib.Path \n",
    "        The original file name to use as a base.\n",
    "    extensions: list-like\n",
    "        A list of file extensions to generate new filenames.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of pathlib.Paths\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Adapted from T. Olsens `file_variations of the pysis module for using pathlib.\n",
    "    \"\"\"\n",
    "    return [Path(filename).with_suffix(extension) for extension in extensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2766e4-724c-4308-88cc-741bcae6f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'abc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e96b5a-6a79-416c-9314-aaa9dbc8bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = \".cub .cal.cub .map.cal.cub\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77119135-7450-4e69-acf7-297218a2052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('abc.cub'), PosixPath('abc.cal.cub'), PosixPath('abc.map.cal.cub')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_variations(fname, extensions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
